---
title: Speeding up rarefaction curves for microbial community ecology
author: ~
date: '2018-12-15'
slug: speeding-up-rarefaction-curves-for-microbial-community-ecology
categories: []
tags: []
image:
  caption: ''
  focal_point: ''
---



<p>When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.</p>
<p>The <code>vegan</code> package in <code>R</code> has a nice function for computing rarefaction curves for species by site abundance tables. However, for microbial datasets this function is often prohibitively slow. This is due to the fact that we often have large numbers of samples, and each sample may contain several thousand sequences, meaning that a large number of random subsamples are taken.</p>
<p>Part of the problem is that the original function only makes use of a single processing core, meaning that random subsamples are computed serially, and each sample is processed serially. This means we could speed the function up by splitting samples across multiple cores. In short, we are parallelising the function.</p>
<p>Below is my attempt to modify the original code into a function that can use multiple processor cores to speed up the calculations.</p>
<pre class="r"><code># you will need to install the parallel package before hand
# and the vegan package if you dont have it
library(vegan)
quickRareCurve &lt;- function (x, step = 1, sample, xlab = &quot;Sample Size&quot;,
  ylab = &quot;Species&quot;, label = TRUE, col, lty, max.cores = T, nCores = 1, ...)
{
    require(parallel)
    x &lt;- as.matrix(x)
    if (!identical(all.equal(x, round(x)), TRUE))
        stop(&quot;function accepts only integers (counts)&quot;)
    if (missing(col))
        col &lt;- par(&quot;col&quot;)
    if (missing(lty))
        lty &lt;- par(&quot;lty&quot;)
    tot &lt;- rowSums(x) # calculates library sizes
    S &lt;- specnumber(x) # calculates n species for each sample
    if (any(S &lt;= 0)) {
        message(&quot;empty rows removed&quot;)
        x &lt;- x[S &gt; 0, , drop = FALSE]
        tot &lt;- tot[S &gt; 0]
        S &lt;- S[S &gt; 0]
    } # removes any empty rows
    nr &lt;- nrow(x) # number of samples
    col &lt;- rep(col, length.out = nr)
    lty &lt;- rep(lty, length.out = nr)
    # parallel mclapply
    # set number of cores
    mc &lt;- getOption(&quot;mc.cores&quot;, ifelse(max.cores, detectCores(), nCores))
    message(paste(&quot;Using &quot;, mc, &quot; cores&quot;))
    out &lt;- mclapply(seq_len(nr), mc.cores = mc, function(i) {
        n &lt;- seq(1, tot[i], by = step)
        if (n[length(n)] != tot[i])
            n &lt;- c(n, tot[i])
        drop(rarefy(x[i, ], n))
    })
    Nmax &lt;- sapply(out, function(x) max(attr(x, &quot;Subsample&quot;)))
    Smax &lt;- sapply(out, max)
     plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = xlab, ylab = ylab,
       type = &quot;n&quot;, ...)
    if (!missing(sample)) {
      abline(v = sample)
      rare &lt;- sapply(out, function(z) approx(x = attr(z, &quot;Subsample&quot;),
         y = z, xout = sample, rule = 1)$y)
      abline(h = rare, lwd = 0.5)
      }
    for (ln in seq_along(out)) {
      N &lt;- attr(out[[ln]], &quot;Subsample&quot;)
      lines(N, out[[ln]], col = col[ln], lty = lty[ln], ...)
      }
    if (label) {
      ordilabel(cbind(tot, S), labels = rownames(x), ...)
      }
    invisible(out)
}</code></pre>
<p>Most of the code is verbatim to the original, but you’ll notice a few extra arguments to specify, and a few extra lines that determine how many cores the function will use.</p>
<p>Essentially, if you do not specify a number of cores, the function will default to using all available cores, which will allow the quickest calculation. Otherwise, you can specify <code>max.cores = F</code>, which will allow you to specify the number of cores using <code>nCores</code>. This is handy if you need some cores to remain usable whilst running the function.</p>
<p>Below is a usage example.</p>
<pre class="r"><code># load some dummy data
data(dune)

quickRareCurve(dune) # will use all cores and print how many cores you have

quickRareCurve(dune, max.cores = F, nCores = 2) # use only 2 cores</code></pre>
<p><img src="/post/2018-12-15-speeding-up-rarefaction-curves-for-microbial-community-ecology_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We can compare the new function’s performance to the original using the <code>microbenchmark</code> package, as below.</p>
<pre class="r"><code>library(microbenchmark)

microbenchmark(rarecurve(dune), quickRareCurve(dune), times = 10)</code></pre>
<p><img src="/post/2018-12-15-speeding-up-rarefaction-curves-for-microbial-community-ecology_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre><code>## Unit: milliseconds
##                  expr      min       lq     mean   median       uq
##       rarecurve(dune) 66.17685 68.48628 69.23183 69.31825 70.03621
##  quickRareCurve(dune) 47.80692 82.52621 80.46054 84.06839 85.01588
##       max neval
##  71.67020    10
##  86.31908    10</code></pre>
<p>As you can see, for this small dataset, the original function is actually quicker. This is because it takes some time to distribute tasks among the cores. However, I’ve found for a typical OTU table (&gt; 50 samples, ~ 15,000 sequences per sample), <code>quickRareCurve</code> can be around 3 times faster.</p>
<p>Feel free to use it as you wish, but I’d be very grateful for any credit if you do use it.</p>
